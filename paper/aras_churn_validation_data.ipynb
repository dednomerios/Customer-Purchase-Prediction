{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 30px; font-weight: bold;\">redaing the data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>SaleTime</th>\n",
       "      <th>art_name</th>\n",
       "      <th>articles_id</th>\n",
       "      <th>price_purchase</th>\n",
       "      <th>price_item</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>BranchCode</th>\n",
       "      <th>BranchName</th>\n",
       "      <th>customers_id</th>\n",
       "      <th>takhfif</th>\n",
       "      <th>gerd</th>\n",
       "      <th>Maliat</th>\n",
       "      <th>Payment channel</th>\n",
       "      <th>Sales channel</th>\n",
       "      <th>factor_id</th>\n",
       "      <th>d_dat</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1395-01-01</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>خانواده پپسي</td>\n",
       "      <td>611</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13803</td>\n",
       "      <td>پاسداران</td>\n",
       "      <td>9120306517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>نقدی</td>\n",
       "      <td>حضوری</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1395-01-01</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>سيب زميني مخصوص</td>\n",
       "      <td>501</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13803</td>\n",
       "      <td>پاسداران</td>\n",
       "      <td>9120306517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>نقدی</td>\n",
       "      <td>حضوری</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1395-01-01</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>لند برگر</td>\n",
       "      <td>101</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>13500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13803</td>\n",
       "      <td>پاسداران</td>\n",
       "      <td>9120306517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>نقدی</td>\n",
       "      <td>حضوری</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1395-01-01</td>\n",
       "      <td>22:36:01</td>\n",
       "      <td>اسموكي برگر</td>\n",
       "      <td>102</td>\n",
       "      <td>14500.0</td>\n",
       "      <td>14500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13804</td>\n",
       "      <td>اندرزگو</td>\n",
       "      <td>9128302458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-205.0</td>\n",
       "      <td>2205.0</td>\n",
       "      <td>نقدی</td>\n",
       "      <td>حضوری</td>\n",
       "      <td>9</td>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1395-01-01</td>\n",
       "      <td>22:36:01</td>\n",
       "      <td>سيب زميني مخصوص</td>\n",
       "      <td>501</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13804</td>\n",
       "      <td>اندرزگو</td>\n",
       "      <td>9128302458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-205.0</td>\n",
       "      <td>2205.0</td>\n",
       "      <td>نقدی</td>\n",
       "      <td>حضوری</td>\n",
       "      <td>9</td>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7284203</th>\n",
       "      <td>1403-05-12</td>\n",
       "      <td>15:51:15</td>\n",
       "      <td>چيکن رپ</td>\n",
       "      <td>903</td>\n",
       "      <td>194500.0</td>\n",
       "      <td>194500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13814</td>\n",
       "      <td>شعبه بازار (پرنسا)</td>\n",
       "      <td>9351555271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>19450.0</td>\n",
       "      <td>نقدی</td>\n",
       "      <td>حضوری</td>\n",
       "      <td>3765492</td>\n",
       "      <td>2024-08-02</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7284204</th>\n",
       "      <td>1403-05-12</td>\n",
       "      <td>15:51:15</td>\n",
       "      <td>زيرو قوطي</td>\n",
       "      <td>604</td>\n",
       "      <td>24500.0</td>\n",
       "      <td>24500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13814</td>\n",
       "      <td>شعبه بازار (پرنسا)</td>\n",
       "      <td>9351555271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>19450.0</td>\n",
       "      <td>نقدی</td>\n",
       "      <td>حضوری</td>\n",
       "      <td>3765492</td>\n",
       "      <td>2024-08-02</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7284205</th>\n",
       "      <td>1403-05-12</td>\n",
       "      <td>16:23:22</td>\n",
       "      <td>لند برگر</td>\n",
       "      <td>101</td>\n",
       "      <td>242500.0</td>\n",
       "      <td>242500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13814</td>\n",
       "      <td>شعبه بازار (پرنسا)</td>\n",
       "      <td>9173058935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24250.0</td>\n",
       "      <td>نقدی</td>\n",
       "      <td>حضوری</td>\n",
       "      <td>3765493</td>\n",
       "      <td>2024-08-02</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7284206</th>\n",
       "      <td>1403-05-12</td>\n",
       "      <td>16:23:22</td>\n",
       "      <td>هي دي استوايي</td>\n",
       "      <td>641</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13814</td>\n",
       "      <td>شعبه بازار (پرنسا)</td>\n",
       "      <td>9173058935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24250.0</td>\n",
       "      <td>نقدی</td>\n",
       "      <td>حضوری</td>\n",
       "      <td>3765493</td>\n",
       "      <td>2024-08-02</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7284207</th>\n",
       "      <td>1403-05-12</td>\n",
       "      <td>14:31:06</td>\n",
       "      <td>سيب زميني مخصوص</td>\n",
       "      <td>501</td>\n",
       "      <td>103500.0</td>\n",
       "      <td>103500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13814</td>\n",
       "      <td>شعبه بازار (پرنسا)</td>\n",
       "      <td>9198883618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13250.0</td>\n",
       "      <td>اسنپ فود انلاين</td>\n",
       "      <td>اسنپ</td>\n",
       "      <td>3765494</td>\n",
       "      <td>2024-08-02</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7284208 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  SaleTime         art_name  articles_id  price_purchase  \\\n",
       "0        1395-01-01  23:59:58     خانواده پپسي          611          2500.0   \n",
       "1        1395-01-01  23:59:58  سيب زميني مخصوص          501          7500.0   \n",
       "2        1395-01-01  23:59:58         لند برگر          101         27000.0   \n",
       "3        1395-01-01  22:36:01      اسموكي برگر          102         14500.0   \n",
       "4        1395-01-01  22:36:01  سيب زميني مخصوص          501          7500.0   \n",
       "...             ...       ...              ...          ...             ...   \n",
       "7284203  1403-05-12  15:51:15          چيکن رپ          903        194500.0   \n",
       "7284204  1403-05-12  15:51:15        زيرو قوطي          604         24500.0   \n",
       "7284205  1403-05-12  16:23:22         لند برگر          101        242500.0   \n",
       "7284206  1403-05-12  16:23:22    هي دي استوايي          641         30000.0   \n",
       "7284207  1403-05-12  14:31:06  سيب زميني مخصوص          501        103500.0   \n",
       "\n",
       "         price_item  Quantity  BranchCode          BranchName  customers_id  \\\n",
       "0              2500       1.0       13803            پاسداران    9120306517   \n",
       "1              7500       1.0       13803            پاسداران    9120306517   \n",
       "2             13500       2.0       13803            پاسداران    9120306517   \n",
       "3             14500       1.0       13804             اندرزگو    9128302458   \n",
       "4              7500       1.0       13804             اندرزگو    9128302458   \n",
       "...             ...       ...         ...                 ...           ...   \n",
       "7284203      194500       1.0       13814  شعبه بازار (پرنسا)    9351555271   \n",
       "7284204       24500       1.0       13814  شعبه بازار (پرنسا)    9351555271   \n",
       "7284205      242500       1.0       13814  شعبه بازار (پرنسا)    9173058935   \n",
       "7284206       30000       1.0       13814  شعبه بازار (پرنسا)    9173058935   \n",
       "7284207      103500       1.0       13814  شعبه بازار (پرنسا)    9198883618   \n",
       "\n",
       "         takhfif   gerd   Maliat  Payment channel Sales channel  factor_id  \\\n",
       "0            0.0 -100.0   3600.0             نقدی         حضوری          6   \n",
       "1            0.0 -100.0   3600.0             نقدی         حضوری          6   \n",
       "2            0.0 -100.0   3600.0             نقدی         حضوری          6   \n",
       "3            0.0 -205.0   2205.0             نقدی         حضوری          9   \n",
       "4            0.0 -205.0   2205.0             نقدی         حضوری          9   \n",
       "...          ...    ...      ...              ...           ...        ...   \n",
       "7284203      0.0   50.0  19450.0             نقدی         حضوری    3765492   \n",
       "7284204      0.0   50.0  19450.0             نقدی         حضوری    3765492   \n",
       "7284205      0.0    0.0  24250.0             نقدی         حضوری    3765493   \n",
       "7284206      0.0    0.0  24250.0             نقدی         حضوری    3765493   \n",
       "7284207      0.0    0.0  13250.0  اسنپ فود انلاين          اسنپ    3765494   \n",
       "\n",
       "              d_dat  group_id  \n",
       "0        2016-03-20       600  \n",
       "1        2016-03-20       500  \n",
       "2        2016-03-20       100  \n",
       "3        2016-03-20       100  \n",
       "4        2016-03-20       500  \n",
       "...             ...       ...  \n",
       "7284203  2024-08-02       900  \n",
       "7284204  2024-08-02       600  \n",
       "7284205  2024-08-02       100  \n",
       "7284206  2024-08-02       600  \n",
       "7284207  2024-08-02       500  \n",
       "\n",
       "[7284208 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'E:\\ARAS\\customer churn paper\\code\\Aras_transactions_new.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 30px; font-weight: bold;\">filtering</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df):\n",
    "    # Select specific columns\n",
    "    df = df[['customers_id', 'price_purchase', 'd_dat', 'group_id']]\n",
    "    \n",
    "    # Convert 'd_dat' to datetime format\n",
    "    df['date'] = pd.to_datetime(df['d_dat'])\n",
    "    \n",
    "    # Drop the original 'd_dat' column\n",
    "    df = df.drop('d_dat', axis=1)\n",
    "    \n",
    "    # Extract the day of the week from 'date'\n",
    "    df['day_of_week'] = df['date'].dt.day_name()\n",
    "    \n",
    "    # Convert 'customers_id' to string\n",
    "    df['customers_id'] = df['customers_id'].astype(str)\n",
    "    \n",
    "    # Filter rows where 'customers_id' starts with '9' and its length is between 8 and 13\n",
    "    df = df[df['customers_id'].str.startswith('9') & df['customers_id'].str.len().between(8, 13)]\n",
    "    \n",
    "    # Convert 'customers_id' and 'group_id' back to appropriate types\n",
    "    df['customers_id'] = df['customers_id'].astype('int64').astype(str)\n",
    "    df['group_id'] = df['group_id'].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = process_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique customers in the original data = 864869\n"
     ]
    }
   ],
   "source": [
    "unique_customer = df['customers_id'].nunique()\n",
    "print(f\"number of unique customers in the original data = {unique_customer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 30px; font-weight: bold;\">creating validation dataframe</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time calculator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of days in 7 months: 213\n"
     ]
    }
   ],
   "source": [
    "# Use this function to estimate the average number of days in a window if neccesary \n",
    "\n",
    "def average_days_in_months(months):\n",
    "    # Average days per month considering leap years\n",
    "    average_days_per_month = (365 * 3 + 366) / (12 * 4)\n",
    "    \n",
    "    # Total days for the given number of months\n",
    "    total_days = average_days_per_month * months\n",
    "    \n",
    "    return round(total_days)\n",
    "\n",
    "given_month = 7   # Enter the number of months as an input to the function \n",
    "\n",
    "days_in_months = average_days_in_months(given_month)\n",
    "print(f\"Average number of days in {given_month} months: {days_in_months}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dataframes(date_start, length_of_each_window, gap_between_each_dataframe, num_df, df):\n",
    "    # Determine the initial start date\n",
    "    date1_start = pd.Timestamp(date_start)\n",
    "    date2_start = df['date'].min()\n",
    "\n",
    "    # Calculate the difference in days to adjust the start date\n",
    "    difference_in_days_start = (date1_start - date2_start).days\n",
    "    start_cutoff_date_0 = df['date'].min() + pd.Timedelta(days=difference_in_days_start)\n",
    "    break_cutoff_date_0 = start_cutoff_date_0 + pd.Timedelta(days=length_of_each_window)\n",
    "\n",
    "    # Initialize lists to store the start and break dates\n",
    "    start_cutoff_dates = [start_cutoff_date_0]\n",
    "    break_cutoff_dates = [break_cutoff_date_0]\n",
    "\n",
    "    # Calculate the subsequent dates for each DataFrame\n",
    "    for i in range(num_df - 1):\n",
    "        next_start_date = start_cutoff_dates[i] + pd.Timedelta(days=gap_between_each_dataframe)\n",
    "        next_break_date = break_cutoff_dates[i] + pd.Timedelta(days=gap_between_each_dataframe)\n",
    "        start_cutoff_dates.append(next_start_date)\n",
    "        break_cutoff_dates.append(next_break_date)\n",
    "        \n",
    "\n",
    "    # Create empty DataFrames\n",
    "    dataframes = [pd.DataFrame() for _ in range(num_df)]\n",
    "\n",
    "    # Filter the DataFrame based on start and break cutoff dates\n",
    "    for i in range(len(break_cutoff_dates)):\n",
    "        dataframes[i] = df[(df['date'] >= start_cutoff_dates[i]) & (df['date'] <= break_cutoff_dates[i])]\n",
    "        globals()[f\"df{i}\"] = dataframes[i]\n",
    "\n",
    "    return start_cutoff_dates, break_cutoff_dates, dataframes\n",
    "\n",
    "length_of_each_window = 213 # Data window length             \n",
    "length_of_the_label_window = 30 # Label window length \n",
    "gap_between_each_dataframe = length_of_each_window + length_of_the_label_window # Data window + Label window \n",
    "\n",
    "num_df = 1\n",
    "\n",
    "total_gap = num_df * gap_between_each_dataframe\n",
    "\n",
    "# Calculate the new start date\n",
    "date_start = df['date'].max() - pd.Timedelta(days=total_gap)\n",
    "\n",
    "start_cutoff_dates, break_cutoff_dates, dfs = create_dataframes(date_start, length_of_each_window, gap_between_each_dataframe, num_df, df)\n",
    "\n",
    "# Assign the data frames\n",
    "df_valid = dfs[0]\n",
    "\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size: 25px; font-weight: bold;\">extracting labels for each dataframe from the original df</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_label_dfs(df, break_cutoff_dates, length_of_the_label_window, num_df):\n",
    "    label_dfs = []\n",
    "    label_lists = []\n",
    "\n",
    "    # Loop through each DataFrame and create the corresponding label DataFrame\n",
    "    for i in range(num_df):\n",
    "        df_label = df[(df['date'] >= break_cutoff_dates[i]) & \n",
    "                      (df['date'] <= break_cutoff_dates[i] + pd.Timedelta(days=length_of_the_label_window))]\n",
    "        label_dfs.append(df_label)\n",
    "        label_lists.append(df_label['customers_id'].tolist())\n",
    "\n",
    "        # Dynamically create a variable name and assign the DataFrame to it\n",
    "        globals()[f'df{i}_label'] = df_label\n",
    "    \n",
    "    return label_dfs, label_lists\n",
    "\n",
    "\n",
    "length_of_the_label_window = length_of_the_label_window  # Set the length of the label window as needed\n",
    "num_df = num_df  # Number of DataFrames\n",
    "\n",
    "label_dfs, label_lists = create_label_dfs(df, break_cutoff_dates, length_of_the_label_window, num_df)\n",
    "\n",
    "df_valid_label = label_dfs[0]\n",
    "\n",
    "df_valid_label_list = label_lists[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-12-03 00:00:00'),\n",
       " Timestamp('2024-07-03 00:00:00'),\n",
       " Timestamp('2024-07-03 00:00:00'),\n",
       " Timestamp('2024-08-02 00:00:00'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid['date'].min(), df_valid['date'].max() , df_valid_label['date'].min(), df_valid_label['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size: 25px; font-weight: bold;\">dropping duplicate rows and also consider only one of customers purchase for each date. dropping 2 or less transactions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    # Drop duplicates based on 'customers_id' and 'date', keeping the first occurrence\n",
    "    df_cleaned = df.drop_duplicates(subset=['customers_id', 'date'], keep='first')\n",
    "\n",
    "    # Identify customers with two or fewer purchases\n",
    "    customers_to_drop = df_cleaned['customers_id'].value_counts()[df_cleaned['customers_id'].value_counts() <= 2].index\n",
    "\n",
    "    # Drop these customers from the DataFrame\n",
    "    df_cleaned = df_cleaned[~df_cleaned['customers_id'].isin(customers_to_drop)]\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "df_valid = clean_dataframe(df_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 30px; font-weight: bold;\">adding feature</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lifetime feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifetimes\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(df, current_date):\n",
    "    metrics_pd = (\n",
    "        lifetimes.utils.summary_data_from_transaction_data(\n",
    "            df,\n",
    "            customer_id_col='customers_id',\n",
    "            datetime_col='date',\n",
    "            observation_period_end=current_date, \n",
    "            freq='D'\n",
    "        )\n",
    "    )\n",
    "    filtered_pd = metrics_pd[metrics_pd['frequency'] > 0]\n",
    "    filtered_pd.reset_index(inplace=True)  # Ensure customers_id is a column\n",
    "    return filtered_pd\n",
    "\n",
    "# Calculate metrics for each subset with different current_date values\n",
    "metrics_valid = calculate_metrics(df_valid, df_valid['date'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifetimes import BetaGeoFitter\n",
    "\n",
    "# Function to fit BG/NBD model and predict expected number of purchases in the next 30 days\n",
    "def predict_expected_purchases(metrics_pd, t=30, penalizer_coef=0.01):\n",
    "    bgf = BetaGeoFitter(penalizer_coef=penalizer_coef)\n",
    "    bgf.fit(metrics_pd['frequency'], metrics_pd['recency'], metrics_pd['T'])\n",
    "    \n",
    "    # Predict the expected number of purchases for the next t days\n",
    "    metrics_pd[f'expected_purchases_{t}'] = bgf.predict(t, metrics_pd['frequency'], metrics_pd['recency'], metrics_pd['T'])\n",
    "    return metrics_pd\n",
    "\n",
    "# Fit BG/NBD model and predict expected number of purchases in the next 30 days\n",
    "metrics_valid = predict_expected_purchases(metrics_valid, t=length_of_the_label_window, penalizer_coef=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = length_of_the_label_window\n",
    "df_valid = pd.merge(df_valid, metrics_valid[['customers_id', 'recency', f'expected_purchases_{t}']], on='customers_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## own features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_features(df, df_name=None, break_cutoff_dates=None):\n",
    "    # Step 1: Add 'purchase_month' feature\n",
    "    df['purchase_month'] = df['date'].dt.to_period('M')\n",
    "    \n",
    "    # Step 2: Add 'most_common_day' feature\n",
    "    def add_most_common_day(df):\n",
    "        def most_common_day(series):\n",
    "            return series.mode().iloc[0]\n",
    "        common_days = df.groupby('customers_id')['day_of_week'].agg(most_common_day).reset_index()\n",
    "        common_days.rename(columns={'day_of_week': 'most_common_day'}, inplace=True)\n",
    "        return df.merge(common_days, on='customers_id')\n",
    "\n",
    "    df = add_most_common_day(df)\n",
    "    \n",
    "    # Step 3: Add 'number_of_purchases' feature\n",
    "    def purchase_count(df):\n",
    "        count_purchase = df['customers_id'].value_counts().rename('number_of_purchases')\n",
    "        return df.merge(count_purchase, left_on='customers_id', right_index=True)\n",
    "\n",
    "    df = purchase_count(df)\n",
    "    \n",
    "    # Step 4: Calculate 'weighted_mean_time_between_purchases'\n",
    "    def calculate_weighted_mean_time_between_purchases(df):\n",
    "        sorted_df = df.sort_values(by=['customers_id', 'date'], ascending=[True, False])\n",
    "        sorted_df['time_diff'] = sorted_df.groupby('customers_id')['date'].diff().dt.days * -1\n",
    "        time_diffs = sorted_df.dropna(subset=['time_diff'])\n",
    "        \n",
    "        def weighted_mean(group):\n",
    "            n = len(group)\n",
    "            weights = [(n - i) ** 2 for i in range(n)]\n",
    "            return np.average(group, weights=weights)\n",
    "\n",
    "        weighted_mean_time_diff = time_diffs.groupby('customers_id')['time_diff'].apply(weighted_mean).reset_index(name='weighted_mean_time_between_purchases')\n",
    "        return df.merge(weighted_mean_time_diff, on='customers_id', how='left').fillna(0)\n",
    "\n",
    "    df = calculate_weighted_mean_time_between_purchases(df)\n",
    "    \n",
    "    # Step 5: Calculate 'std_between_purchase'\n",
    "    def calculate_std_features(df):\n",
    "        sorted_df = df.sort_values(by=['customers_id', 'date'], ascending=[True, False])\n",
    "        sorted_df['time_diff'] = sorted_df.groupby('customers_id')['date'].diff().dt.days * -1\n",
    "\n",
    "        def calculate_std_between_purchase(group):\n",
    "            n = len(group)\n",
    "            if n == 1:\n",
    "                return 0\n",
    "            weights = [(n - i) ** 2 for i in range(1, n)]\n",
    "            weight_sum = sum(weights)\n",
    "            normalized_weights = [w / weight_sum for w in weights]\n",
    "            weighted_mean = np.dot(group[1:], normalized_weights)\n",
    "            squared_differences = ((group[1:] - weighted_mean) ** 2)\n",
    "            return np.sqrt(np.dot(squared_differences, normalized_weights))\n",
    "\n",
    "        std_between_purchase = sorted_df.groupby('customers_id')['time_diff'].apply(calculate_std_between_purchase).reset_index(name='std_between_purchase')\n",
    "        return df.merge(std_between_purchase, on='customers_id', how='left')\n",
    "\n",
    "    df = calculate_std_features(df)\n",
    "    \n",
    "    # Step 6: Calculate 'max_time_without_purchase'\n",
    "    def calculate_max_without_purchase(df):\n",
    "        sorted_df = df.sort_values(by=['customers_id', 'date'], ascending=[True, False])\n",
    "        sorted_df['time_diff'] = sorted_df.groupby('customers_id')['date'].diff().dt.days * -1\n",
    "        max_time_without_purchase = sorted_df.groupby('customers_id')['time_diff'].max().reset_index(name='max_time_without_purchase')\n",
    "        return df.merge(max_time_without_purchase, on='customers_id', how='left')\n",
    "\n",
    "    df = calculate_max_without_purchase(df)\n",
    "    \n",
    "    # Time since last purchase in months \n",
    "    # # Step 7: Conditional 'calculate_time_since_last_purchase'\n",
    "    # def calculate_time_since_last_purchase(df, index):\n",
    "    #     last_purchase_date = df.groupby('customers_id')['date'].max().reset_index(name='last_purchase_date')\n",
    "    #     specific_date = break_cutoff_dates[index]\n",
    "    #     last_purchase_date['time_since_last_purchase'] = (\n",
    "    #         (specific_date.year - last_purchase_date['last_purchase_date'].dt.year) * 12 +\n",
    "    #         (specific_date.month - last_purchase_date['last_purchase_date'].dt.month)\n",
    "    #     )\n",
    "    #     return df.merge(last_purchase_date[['customers_id', 'time_since_last_purchase']], on='customers_id', how='left')\n",
    "    \n",
    "    # Time since last purchase in days \n",
    "    def calculate_time_since_last_purchase(df, index):\n",
    "        # Get the last purchase date for each customer\n",
    "        last_purchase_date = df.groupby('customers_id')['date'].max().reset_index(name='last_purchase_date')\n",
    "        \n",
    "        # Define the specific date (cutoff date)\n",
    "        specific_date = break_cutoff_dates[index]\n",
    "        \n",
    "        # Calculate the difference in days\n",
    "        last_purchase_date['time_since_last_purchase'] = (specific_date - last_purchase_date['last_purchase_date']).dt.days\n",
    "        \n",
    "        # Merge the result back into the original DataFrame\n",
    "        return df.merge(last_purchase_date[['customers_id', 'time_since_last_purchase']], on='customers_id', how='left')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Apply the correct time_since_last_purchase calculation based on df_name\n",
    "    if df_name is not None:\n",
    "        index_mapping = {'df_valid': 0}\n",
    "        if df_name in index_mapping:\n",
    "            df = calculate_time_since_last_purchase(df, index_mapping[df_name])\n",
    "\n",
    "    # Step 8: Conditional 'calculate_time_since_first_purchase'\n",
    "    def calculate_time_since_first_purchase(df, index):\n",
    "        first_purchase_date = df.groupby('customers_id')['date'].min().reset_index(name='first_purchase_date')\n",
    "        specific_date = break_cutoff_dates[index]\n",
    "        first_purchase_date['time_since_first_purchase'] = (specific_date - first_purchase_date['first_purchase_date']).dt.days\n",
    "        return df.merge(first_purchase_date[['customers_id', 'time_since_first_purchase']], on='customers_id', how='left')\n",
    "\n",
    "    # Apply the correct time_since_first_purchase calculation based on df_name\n",
    "    if df_name is not None:\n",
    "        if df_name in index_mapping:\n",
    "            df = calculate_time_since_first_purchase(df, index_mapping[df_name])\n",
    "\n",
    "    # Step 9: Calculate 'transaction recency'\n",
    "    def calculate_transaction_recency(df):\n",
    "        num_unique_customers = df['customers_id'].nunique()\n",
    "        last_purchase_date = df.groupby('customers_id')['date'].max().reset_index(name='last_purchase_date')\n",
    "        last_purchase_date['time_since_last_purchase'] = df['time_since_last_purchase']\n",
    "        last_purchase_date['transaction recency'] = last_purchase_date['time_since_last_purchase'] / num_unique_customers\n",
    "        return df.merge(last_purchase_date[['customers_id', 'transaction recency']], on='customers_id', how='left')\n",
    "\n",
    "    df = calculate_transaction_recency(df)\n",
    "    \n",
    "    # Step 10: Add one-hot encoded features for 'time_since_last_purchase'\n",
    "    def add_onehot_last_purchase(df, threshold):\n",
    "        upper_limit = threshold + 30\n",
    "        new_column_name = f'last_between_{threshold}_and_{upper_limit}'\n",
    "        df[new_column_name] = ((df['time_since_last_purchase'] >= threshold) & (df['time_since_last_purchase'] < upper_limit)).astype(int)\n",
    "        return df\n",
    "\n",
    "    thresholds = [0, 30, 60]\n",
    "    for threshold in thresholds:\n",
    "        df = add_onehot_last_purchase(df, threshold)\n",
    "    \n",
    "    # Step 11: Calculate 'diff_last_and_penultimate'\n",
    "    def calculate_time_between_last_two_purchases(df):\n",
    "        if df['date'].dtype == 'object':\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "        df_sorted = df.sort_values(by=['customers_id', 'date'])\n",
    "        grouped_dates = df_sorted.groupby('customers_id')['date'].apply(list)\n",
    "        diff_last_and_penultimate = grouped_dates.apply(lambda dates: (dates[-1] - dates[-2]).days if len(dates) > 1 else None)\n",
    "        diff_last_and_penultimate = diff_last_and_penultimate.reset_index(name='diff_last_and_penultimate')\n",
    "        return df.merge(diff_last_and_penultimate, on='customers_id', how='left')\n",
    "\n",
    "    df = calculate_time_between_last_two_purchases(df)\n",
    "    \n",
    "    # Step 12: Calculate 'diff_penultimate_and_previous'\n",
    "    def calculate_time_between_penultimate_and_previous(df):\n",
    "        if df['date'].dtype == 'object':\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "        df_sorted = df.sort_values(by=['customers_id', 'date'])  # Corrected line\n",
    "        grouped_dates = df_sorted.groupby('customers_id')['date'].apply(list)\n",
    "        diff_penultimate_and_previous = grouped_dates.apply(lambda dates: (dates[-2] - dates[-3]).days if len(dates) > 2 else None)\n",
    "        diff_penultimate_and_previous = diff_penultimate_and_previous.reset_index(name='diff_penultimate_and_previous')\n",
    "        return df.merge(diff_penultimate_and_previous, on='customers_id', how='left')\n",
    "\n",
    "    df = calculate_time_between_penultimate_and_previous(df)\n",
    "\n",
    "    \n",
    "    # Step 13: Add 'threshold' features\n",
    "    def add_threshold_features(df):\n",
    "        h_1 = 2\n",
    "        h_2 = 4\n",
    "        h_3 = 8\n",
    "        df['tresh_1'] = df['weighted_mean_time_between_purchases'] + h_1 * df['std_between_purchase']\n",
    "        df['tresh_2'] = df['weighted_mean_time_between_purchases'] + h_2 * df['std_between_purchase']\n",
    "        df['tresh_3'] = df['weighted_mean_time_between_purchases'] + h_3 * df['std_between_purchase']\n",
    "        return df\n",
    "\n",
    "    df = add_threshold_features(df)\n",
    "\n",
    "    # Step 14: Frequency classification\n",
    "    def classify_frequency(row):\n",
    "        if row['time_since_last_purchase'] <= row['tresh_1']:\n",
    "            return 'normal'\n",
    "        elif row['time_since_last_purchase'] <= row['tresh_2']:\n",
    "            return 'attrition'\n",
    "        elif row['time_since_last_purchase'] <= row['tresh_3']:\n",
    "            return 'at-risk'\n",
    "        else:\n",
    "            return 'lost'\n",
    "\n",
    "    def frequency_classification(df):\n",
    "        df['freq_class'] = df.apply(classify_frequency, axis=1)\n",
    "        return df\n",
    "\n",
    "    df = frequency_classification(df)\n",
    "\n",
    "    # Step 15: Add moving average features\n",
    "    def add_moving_average_purchase_feature(df, window_size, column_name):\n",
    "        df_sorted = df.sort_values(by=['customers_id', 'date']).copy()\n",
    "        df[column_name] = df_sorted.groupby('customers_id')['price_purchase'].transform(\n",
    "            lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
    "        return df\n",
    "\n",
    "    df = add_moving_average_purchase_feature(df, 3, 'rolling_avg_3')\n",
    "    df = add_moving_average_purchase_feature(df, 6, 'rolling_avg_6')\n",
    "\n",
    "    # Step 16: Add binned purchase feature\n",
    "    def add_binned_purchase_feature(df):\n",
    "        purchase_sums = df.groupby('customers_id')['price_purchase'].sum().reset_index(name='binned_purchase')\n",
    "        return pd.merge(df, purchase_sums, on='customers_id', how='left')\n",
    "\n",
    "    df = add_binned_purchase_feature(df)\n",
    "\n",
    "    # Step 17: Calculate relative change\n",
    "    def calculate_relative_change(df):\n",
    "        df_sorted = df.sort_values(by=['customers_id', 'date'])\n",
    "        if 'rolling_avg_6' not in df.columns:\n",
    "            df_sorted['rolling_avg_6'] = df_sorted.groupby('customers_id')['price_purchase'].transform(\n",
    "                lambda x: x.rolling(window=6, min_periods=1).mean())\n",
    "        df_sorted['rolling_avg_6'].fillna(0, inplace=True)\n",
    "        last_price_purchase = df_sorted.groupby('customers_id')['price_purchase'].last().fillna(0)\n",
    "        sixth_last_rolling_avg = df_sorted.groupby('customers_id').apply(\n",
    "            lambda x: x['rolling_avg_6'].iloc[-6] if len(x) >= 6 else x['rolling_avg_6'].iloc[0]).fillna(0)\n",
    "        relative_change = (last_price_purchase - sixth_last_rolling_avg) / sixth_last_rolling_avg.replace(0, 1)\n",
    "        df['d'] = df['customers_id'].map(relative_change)\n",
    "        return df\n",
    "\n",
    "    df = calculate_relative_change(df)\n",
    "\n",
    "    # Step 18: Purchase trend classification\n",
    "    def purchase_trend_classifier(row):\n",
    "        a1 = 0.15\n",
    "        a2 = 0.225\n",
    "        a3 = 0.3\n",
    "        if row['d'] <= -a3:\n",
    "            return 'decreasing--'\n",
    "        elif -a3 < row['d'] <= -a2:\n",
    "            return 'decreasing-'\n",
    "        elif -a2 < row['d'] <= -a1:\n",
    "            return 'decreasing'\n",
    "        elif -a1 < row['d'] <= a1:\n",
    "            return 'stable'\n",
    "        elif a1 < row['d'] <= a2:\n",
    "            return 'increasing'\n",
    "        elif a2 < row['d'] <= a3:\n",
    "            return 'increasing+'\n",
    "        elif a3 < row['d']:\n",
    "            return 'increasing++'\n",
    "\n",
    "    def purchase_trend(df):\n",
    "        df['purchase_trend'] = df.apply(purchase_trend_classifier, axis=1)\n",
    "        return df\n",
    "\n",
    "    df = purchase_trend(df)\n",
    "\n",
    "    # Step 19: Add purchase value features (max, mean, median)\n",
    "    def add_max_purchase_feature(df):\n",
    "        max_purchase = df.groupby('customers_id')['price_purchase'].max().reset_index(name='max_purchase')\n",
    "        return pd.merge(df, max_purchase, on='customers_id', how='left')\n",
    "\n",
    "    def add_mean_purchase_feature(df):\n",
    "        mean_purchase = df.groupby('customers_id')['price_purchase'].mean().reset_index(name='mean_purchase')\n",
    "        return pd.merge(df, mean_purchase, on='customers_id', how='left')\n",
    "\n",
    "    def add_median_purchase_feature(df):\n",
    "        median_purchase = df.groupby('customers_id')['price_purchase'].median().reset_index(name='median_purchase')\n",
    "        return pd.merge(df, median_purchase, on='customers_id', how='left')\n",
    "\n",
    "    df = add_max_purchase_feature(df)\n",
    "    df = add_mean_purchase_feature(df)\n",
    "    df = add_median_purchase_feature(df)\n",
    "\n",
    "    # Step 20: Calculate relative purchase value\n",
    "    def calculate_relative_purchase_value(df):\n",
    "        df = df.sort_values(['customers_id', 'date'])\n",
    "        rel_purchase_values = df.groupby('customers_id').apply(lambda group: calculate_rel_value(group))\n",
    "        return df.merge(rel_purchase_values.rename('rel_purchase_value'), on='customers_id', how='left')\n",
    "\n",
    "    def calculate_rel_value(group):\n",
    "        if len(group) < 5:\n",
    "            first_value = group.iloc[0]['price_purchase']\n",
    "            last_value = group.iloc[-1]['price_purchase']\n",
    "            if first_value == 0:\n",
    "                return np.nan\n",
    "            return (last_value - first_value) / first_value\n",
    "        else:\n",
    "            fifth_from_last_value = group.iloc[-5]['price_purchase']\n",
    "            last_value = group.iloc[-1]['price_purchase']\n",
    "            if fifth_from_last_value == 0:\n",
    "                return np.nan\n",
    "            return (last_value - fifth_from_last_value) / fifth_from_last_value\n",
    "\n",
    "    df = calculate_relative_purchase_value(df)\n",
    "    df['rel_purchase_value'].fillna(0, inplace=True)\n",
    "\n",
    "    # Step 21: Time frame classification\n",
    "    def time_frame_var_row(row):\n",
    "        mu = 0.3\n",
    "        if row['rel_purchase_value'] < -mu:\n",
    "            return 'steady'\n",
    "        elif abs(row['rel_purchase_value']) <= mu:\n",
    "            return 'within-limits'\n",
    "        elif row['rel_purchase_value'] > mu:\n",
    "            return 'altering'\n",
    "\n",
    "    def time_frame_char(df):\n",
    "        df['time_frame_char'] = df.apply(time_frame_var_row, axis=1)\n",
    "        return df\n",
    "\n",
    "    df = time_frame_char(df)\n",
    "\n",
    "    # Step 22: Add year, month, and day features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "df_valid = generate_features(df_valid, df_name='df_valid', break_cutoff_dates=break_cutoff_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size: 25px; font-weight: bold;\">make a list of features </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customers_id', 'price_purchase', 'group_id', 'date', 'day_of_week', 'recency', 'expected_purchases_30', 'purchase_month', 'most_common_day', 'number_of_purchases', 'weighted_mean_time_between_purchases', 'std_between_purchase', 'max_time_without_purchase', 'time_since_last_purchase', 'time_since_first_purchase', 'transaction recency', 'last_between_0_and_30', 'last_between_30_and_60', 'last_between_60_and_90', 'diff_last_and_penultimate', 'diff_penultimate_and_previous', 'tresh_1', 'tresh_2', 'tresh_3', 'freq_class', 'rolling_avg_3', 'rolling_avg_6', 'binned_purchase', 'd', 'purchase_trend', 'max_purchase', 'mean_purchase', 'median_purchase', 'rel_purchase_value', 'time_frame_char', 'year', 'month', 'day']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list = df_valid.columns.tolist()\n",
    "print(features_list)\n",
    "len(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177390, 38)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 30px; font-weight: bold;\">adding label</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_df_with_list(df, customer_ids_list):\n",
    "    # Create a new DataFrame to avoid modifying the original one\n",
    "    labeled_df = df.copy()\n",
    "    \n",
    "    # Create a new column 'label' in labeled_df and initialize it with 0\n",
    "    labeled_df['label'] = 0\n",
    "    \n",
    "    # Set label as 1 for customer IDs present in customer_ids_list\n",
    "    labeled_df.loc[labeled_df['customers_id'].isin(customer_ids_list), 'label'] = 1\n",
    "    \n",
    "    return labeled_df\n",
    "\n",
    "df_valid_labled = label_df_with_list(df_valid, df_valid_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['customers_id', 'price_purchase', 'group_id', 'date', 'day_of_week',\n",
      "       'recency', 'expected_purchases_30', 'purchase_month', 'most_common_day',\n",
      "       'number_of_purchases', 'weighted_mean_time_between_purchases',\n",
      "       'std_between_purchase', 'max_time_without_purchase',\n",
      "       'time_since_last_purchase', 'time_since_first_purchase',\n",
      "       'transaction recency', 'last_between_0_and_30',\n",
      "       'last_between_30_and_60', 'last_between_60_and_90',\n",
      "       'diff_last_and_penultimate', 'diff_penultimate_and_previous', 'tresh_1',\n",
      "       'tresh_2', 'tresh_3', 'freq_class', 'rolling_avg_3', 'rolling_avg_6',\n",
      "       'binned_purchase', 'd', 'purchase_trend', 'max_purchase',\n",
      "       'mean_purchase', 'median_purchase', 'rel_purchase_value',\n",
      "       'time_frame_char', 'year', 'month', 'day'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_valid.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_valid.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "print(set(df_valid_labled['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 30px; font-weight: bold;\">preparing the data for the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customers_id</th>\n",
       "      <th>price_purchase</th>\n",
       "      <th>group_id</th>\n",
       "      <th>date</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>recency</th>\n",
       "      <th>expected_purchases_30</th>\n",
       "      <th>purchase_month</th>\n",
       "      <th>most_common_day</th>\n",
       "      <th>number_of_purchases</th>\n",
       "      <th>...</th>\n",
       "      <th>purchase_trend</th>\n",
       "      <th>max_purchase</th>\n",
       "      <th>mean_purchase</th>\n",
       "      <th>median_purchase</th>\n",
       "      <th>rel_purchase_value</th>\n",
       "      <th>time_frame_char</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9001601666</td>\n",
       "      <td>342000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2022-03-10</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.484710</td>\n",
       "      <td>2022-03</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>decreasing--</td>\n",
       "      <td>342000.0</td>\n",
       "      <td>173500.000000</td>\n",
       "      <td>148500.0</td>\n",
       "      <td>-0.912281</td>\n",
       "      <td>steady</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9001601666</td>\n",
       "      <td>148500.0</td>\n",
       "      <td>500</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>Friday</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.484710</td>\n",
       "      <td>2022-06</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>decreasing--</td>\n",
       "      <td>342000.0</td>\n",
       "      <td>173500.000000</td>\n",
       "      <td>148500.0</td>\n",
       "      <td>-0.912281</td>\n",
       "      <td>steady</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9001601666</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>600</td>\n",
       "      <td>2022-06-14</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.484710</td>\n",
       "      <td>2022-06</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>decreasing--</td>\n",
       "      <td>342000.0</td>\n",
       "      <td>173500.000000</td>\n",
       "      <td>148500.0</td>\n",
       "      <td>-0.912281</td>\n",
       "      <td>steady</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9002503077</td>\n",
       "      <td>108500.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.163736</td>\n",
       "      <td>2022-05</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>stable</td>\n",
       "      <td>158500.0</td>\n",
       "      <td>104500.000000</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>-0.064516</td>\n",
       "      <td>within-limits</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9002503077</td>\n",
       "      <td>158500.0</td>\n",
       "      <td>150</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>Friday</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.163736</td>\n",
       "      <td>2022-06</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>stable</td>\n",
       "      <td>158500.0</td>\n",
       "      <td>104500.000000</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>-0.064516</td>\n",
       "      <td>within-limits</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680795</th>\n",
       "      <td>9999912003</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>600</td>\n",
       "      <td>2024-06-11</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.770284</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>decreasing--</td>\n",
       "      <td>159500.0</td>\n",
       "      <td>100166.666667</td>\n",
       "      <td>94500.0</td>\n",
       "      <td>-0.928177</td>\n",
       "      <td>steady</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680796</th>\n",
       "      <td>9999979878</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>600</td>\n",
       "      <td>2024-02-09</td>\n",
       "      <td>Friday</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.588324</td>\n",
       "      <td>2024-02</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>increasing++</td>\n",
       "      <td>350500.0</td>\n",
       "      <td>121787.500000</td>\n",
       "      <td>58575.0</td>\n",
       "      <td>3.776923</td>\n",
       "      <td>altering</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680797</th>\n",
       "      <td>9999979878</td>\n",
       "      <td>350500.0</td>\n",
       "      <td>190</td>\n",
       "      <td>2024-04-19</td>\n",
       "      <td>Friday</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.588324</td>\n",
       "      <td>2024-04</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>increasing++</td>\n",
       "      <td>350500.0</td>\n",
       "      <td>121787.500000</td>\n",
       "      <td>58575.0</td>\n",
       "      <td>3.776923</td>\n",
       "      <td>altering</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680798</th>\n",
       "      <td>9999979878</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>600</td>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>Friday</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.588324</td>\n",
       "      <td>2024-05</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>increasing++</td>\n",
       "      <td>350500.0</td>\n",
       "      <td>121787.500000</td>\n",
       "      <td>58575.0</td>\n",
       "      <td>3.776923</td>\n",
       "      <td>altering</td>\n",
       "      <td>2024</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680799</th>\n",
       "      <td>9999979878</td>\n",
       "      <td>93150.0</td>\n",
       "      <td>500</td>\n",
       "      <td>2024-07-12</td>\n",
       "      <td>Friday</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.588324</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>increasing++</td>\n",
       "      <td>350500.0</td>\n",
       "      <td>121787.500000</td>\n",
       "      <td>58575.0</td>\n",
       "      <td>3.776923</td>\n",
       "      <td>altering</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>680800 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       customers_id  price_purchase group_id       date day_of_week  recency  \\\n",
       "0        9001601666        342000.0      100 2022-03-10    Thursday     96.0   \n",
       "1        9001601666        148500.0      500 2022-06-03      Friday     96.0   \n",
       "2        9001601666         30000.0      600 2022-06-14     Tuesday     96.0   \n",
       "3        9002503077        108500.0      100 2022-05-26    Thursday     44.0   \n",
       "4        9002503077        158500.0      150 2022-06-10      Friday     44.0   \n",
       "...             ...             ...      ...        ...         ...      ...   \n",
       "680795   9999912003          6500.0      600 2024-06-11     Tuesday    149.0   \n",
       "680796   9999979878         19500.0      600 2024-02-09      Friday    154.0   \n",
       "680797   9999979878        350500.0      190 2024-04-19      Friday    154.0   \n",
       "680798   9999979878         24000.0      600 2024-05-10      Friday    154.0   \n",
       "680799   9999979878         93150.0      500 2024-07-12      Friday    154.0   \n",
       "\n",
       "        expected_purchases_30 purchase_month most_common_day  \\\n",
       "0                    0.484710        2022-03          Friday   \n",
       "1                    0.484710        2022-06          Friday   \n",
       "2                    0.484710        2022-06          Friday   \n",
       "3                    1.163736        2022-05          Friday   \n",
       "4                    1.163736        2022-06          Friday   \n",
       "...                       ...            ...             ...   \n",
       "680795               0.770284        2024-06          Sunday   \n",
       "680796               0.588324        2024-02          Friday   \n",
       "680797               0.588324        2024-04          Friday   \n",
       "680798               0.588324        2024-05          Friday   \n",
       "680799               0.588324        2024-07          Friday   \n",
       "\n",
       "        number_of_purchases  ...  purchase_trend  max_purchase  mean_purchase  \\\n",
       "0                         3  ...    decreasing--      342000.0  173500.000000   \n",
       "1                         3  ...    decreasing--      342000.0  173500.000000   \n",
       "2                         3  ...    decreasing--      342000.0  173500.000000   \n",
       "3                         4  ...          stable      158500.0  104500.000000   \n",
       "4                         4  ...          stable      158500.0  104500.000000   \n",
       "...                     ...  ...             ...           ...            ...   \n",
       "680795                    6  ...    decreasing--      159500.0  100166.666667   \n",
       "680796                    4  ...    increasing++      350500.0  121787.500000   \n",
       "680797                    4  ...    increasing++      350500.0  121787.500000   \n",
       "680798                    4  ...    increasing++      350500.0  121787.500000   \n",
       "680799                    4  ...    increasing++      350500.0  121787.500000   \n",
       "\n",
       "        median_purchase  rel_purchase_value  time_frame_char  year  month  \\\n",
       "0              148500.0           -0.912281           steady  2022      3   \n",
       "1              148500.0           -0.912281           steady  2022      6   \n",
       "2              148500.0           -0.912281           steady  2022      6   \n",
       "3              105000.0           -0.064516    within-limits  2022      5   \n",
       "4              105000.0           -0.064516    within-limits  2022      6   \n",
       "...                 ...                 ...              ...   ...    ...   \n",
       "680795          94500.0           -0.928177           steady  2024      6   \n",
       "680796          58575.0            3.776923         altering  2024      2   \n",
       "680797          58575.0            3.776923         altering  2024      4   \n",
       "680798          58575.0            3.776923         altering  2024      5   \n",
       "680799          58575.0            3.776923         altering  2024      7   \n",
       "\n",
       "        day  label  \n",
       "0        10      0  \n",
       "1         3      0  \n",
       "2        14      0  \n",
       "3        26      0  \n",
       "4        10      0  \n",
       "...     ...    ...  \n",
       "680795   11      0  \n",
       "680796    9      0  \n",
       "680797   19      0  \n",
       "680798   10      0  \n",
       "680799   12      0  \n",
       "\n",
       "[680800 rows x 39 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # List of DataFrames\n",
    "# dfs = [df_valid_labled]\n",
    "\n",
    "# # Concatenate all DataFrames\n",
    "# concat_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_df = pd.merge(concat_df, final_df[['customers_id', 'final_cluster']], on='customers_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82160"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat_df['customers_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_valid_labled['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size: 25px; font-weight: bold;\">data shuffling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_labled = df_valid_labled.sample(frac=1, random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size: 25px; font-weight: bold;\">defining X and y</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((177390, 38), (177390,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_valid_labled.drop(['label'], axis = 1)\n",
    "y = df_valid_labled['label']\n",
    "X.shape, y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    97756\n",
       "1    79634\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: red; font-size: 25px; font-weight: bold;\">Transforming categorical data using LabelEncoder</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customers_id', 'group_id', 'day_of_week', 'most_common_day', 'freq_class', 'purchase_trend', 'time_frame_char']\n"
     ]
    }
   ],
   "source": [
    "object_columns = X.select_dtypes(include=['object'])\n",
    "object_column_names = object_columns.columns.tolist()\n",
    "print(object_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((177390, 32), (177390,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoders for each categorical column\n",
    "label_encoder_freq_class = LabelEncoder()\n",
    "label_encoder_time_frame_char = LabelEncoder()\n",
    "label_encoder_purchase_trend = LabelEncoder()\n",
    "label_encoder_day_of_week = LabelEncoder()\n",
    "label_encoder_most_common_day = LabelEncoder()\n",
    "\n",
    "# Fit the label encoders and transform the data\n",
    "X['freq_class_encoded'] = label_encoder_freq_class.fit_transform(X['freq_class'])\n",
    "X['time_frame_char_encoded'] = label_encoder_time_frame_char.fit_transform(X['time_frame_char'])\n",
    "X['purchase_trend_encoded'] = label_encoder_purchase_trend.fit_transform(X['purchase_trend'])\n",
    "X['day_of_week_encoded'] = label_encoder_day_of_week.fit_transform(X['day_of_week'])\n",
    "X['most_common_day_encoded'] = label_encoder_most_common_day.fit_transform(X['most_common_day'])\n",
    "\n",
    "# Drop the original categorical columns\n",
    "X = X.drop(['freq_class', 'time_frame_char', 'purchase_trend', 'day_of_week', 'most_common_day'], axis=1)\n",
    "\n",
    "# Drop unneccessary features like reshholds etc. \n",
    "X = X.drop(['tresh_1', 'tresh_2', 'tresh_3', 'd', 'purchase_month', 'date'], axis=1)\n",
    "\n",
    "# Output the shape of X_train and y_train to ensure consistency\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 177390 entries, 0 to 177389\n",
      "Data columns (total 32 columns):\n",
      " #   Column                                Non-Null Count   Dtype  \n",
      "---  ------                                --------------   -----  \n",
      " 0   customers_id                          177390 non-null  object \n",
      " 1   price_purchase                        177390 non-null  float64\n",
      " 2   group_id                              177390 non-null  object \n",
      " 3   recency                               177390 non-null  float64\n",
      " 4   expected_purchases_30                 177390 non-null  float64\n",
      " 5   number_of_purchases                   177390 non-null  int64  \n",
      " 6   weighted_mean_time_between_purchases  177390 non-null  float64\n",
      " 7   std_between_purchase                  177390 non-null  float64\n",
      " 8   max_time_without_purchase             177390 non-null  float64\n",
      " 9   time_since_last_purchase              177390 non-null  int64  \n",
      " 10  time_since_first_purchase             177390 non-null  int64  \n",
      " 11  transaction recency                   177390 non-null  float64\n",
      " 12  last_between_0_and_30                 177390 non-null  int32  \n",
      " 13  last_between_30_and_60                177390 non-null  int32  \n",
      " 14  last_between_60_and_90                177390 non-null  int32  \n",
      " 15  diff_last_and_penultimate             177390 non-null  int64  \n",
      " 16  diff_penultimate_and_previous         177390 non-null  int64  \n",
      " 17  rolling_avg_3                         177390 non-null  float64\n",
      " 18  rolling_avg_6                         177390 non-null  float64\n",
      " 19  binned_purchase                       177390 non-null  float64\n",
      " 20  max_purchase                          177390 non-null  float64\n",
      " 21  mean_purchase                         177390 non-null  float64\n",
      " 22  median_purchase                       177390 non-null  float64\n",
      " 23  rel_purchase_value                    177390 non-null  float64\n",
      " 24  year                                  177390 non-null  int32  \n",
      " 25  month                                 177390 non-null  int32  \n",
      " 26  day                                   177390 non-null  int32  \n",
      " 27  freq_class_encoded                    177390 non-null  int32  \n",
      " 28  time_frame_char_encoded               177390 non-null  int32  \n",
      " 29  purchase_trend_encoded                177390 non-null  int32  \n",
      " 30  day_of_week_encoded                   177390 non-null  int32  \n",
      " 31  most_common_day_encoded               177390 non-null  int32  \n",
      "dtypes: float64(14), int32(11), int64(5), object(2)\n",
      "memory usage: 35.9+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size: 25px; font-weight: bold;\">scaling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Specify the columns you don't want to scale\n",
    "exclude_columns = ['customers_id', 'group_id']\n",
    "\n",
    "# Create a copy of X to hold the scaled values\n",
    "X_scaled = X.copy()\n",
    "\n",
    "# Apply MinMaxScaler to the selected columns\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_scale = [col for col in X.columns if col not in exclude_columns]\n",
    "\n",
    "# Scale only the columns that are not in the exclude list\n",
    "X_scaled[columns_to_scale] = scaler.fit_transform(X[columns_to_scale])\n",
    "\n",
    "# X_scaled now contains the scaled columns along with the unchanged customer_id and group_id columns\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create X_clus dataframe with only the specified attributes for clustering\n",
    "X_clus = X_scaled[['number_of_purchases', 'weighted_mean_time_between_purchases',\n",
    "            'std_between_purchase', 'time_since_last_purchase', 'binned_purchase', 'max_purchase', 'month',\n",
    "            f'expected_purchases_{length_of_the_label_window}', 'recency', 'time_since_first_purchase']]\n",
    "\n",
    "# Apply KMeans to the entire dataset\n",
    "kmeans = KMeans(n_clusters=9, random_state=42)\n",
    "X_clus['final_cluster'] = kmeans.fit_predict(X_clus)\n",
    "\n",
    "# Merge the 'segment' column back into the original X dataframe\n",
    "X_scaled = X_scaled.join(X_clus['final_cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 30px; font-weight: bold;\">cluster_id</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_features = X_scaled[['customers_id', 'group_id']]\n",
    "\n",
    "# Determine optimal number of clusters based on visualizations\n",
    "k_optimal = 3  # Replace with the optimal number determined\n",
    "\n",
    "# Apply K-Means algorithm\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "X_scaled['segment_id'] = kmeans.fit_predict(X_features)\n",
    "\n",
    "X_scaled.drop(['customers_id', 'group_id'], axis = 1, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = X_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['price_purchase', 'recency', 'expected_purchases_30',\n",
      "       'number_of_purchases', 'weighted_mean_time_between_purchases',\n",
      "       'std_between_purchase', 'max_time_without_purchase',\n",
      "       'time_since_last_purchase', 'time_since_first_purchase',\n",
      "       'transaction recency', 'last_between_0_and_30',\n",
      "       'last_between_30_and_60', 'last_between_60_and_90',\n",
      "       'diff_last_and_penultimate', 'diff_penultimate_and_previous',\n",
      "       'rolling_avg_3', 'rolling_avg_6', 'binned_purchase', 'max_purchase',\n",
      "       'mean_purchase', 'median_purchase', 'rel_purchase_value', 'year',\n",
      "       'month', 'day', 'freq_class_encoded', 'time_frame_char_encoded',\n",
      "       'purchase_trend_encoded', 'day_of_week_encoded',\n",
      "       'most_common_day_encoded', 'final_cluster', 'segment_id'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_list)\n",
    "len(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract the 'final_cluster' column\n",
    "final_cluster = X_scaled.iloc[:, 30]\n",
    "\n",
    "# Step 2: Drop the 'final_cluster' column from its current position\n",
    "X_scaled = X_scaled.drop(X_scaled.columns[30], axis=1)\n",
    "\n",
    "# Step 3: Insert the 'final_cluster' column at the 25th position\n",
    "X_scaled.insert(25, 'final_cluster', final_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['price_purchase', 'recency', 'expected_purchases_30',\n",
      "       'number_of_purchases', 'weighted_mean_time_between_purchases',\n",
      "       'std_between_purchase', 'max_time_without_purchase',\n",
      "       'time_since_last_purchase', 'time_since_first_purchase',\n",
      "       'transaction recency', 'last_between_0_and_30',\n",
      "       'last_between_30_and_60', 'last_between_60_and_90',\n",
      "       'diff_last_and_penultimate', 'diff_penultimate_and_previous',\n",
      "       'rolling_avg_3', 'rolling_avg_6', 'binned_purchase', 'max_purchase',\n",
      "       'mean_purchase', 'median_purchase', 'rel_purchase_value', 'year',\n",
      "       'month', 'day', 'final_cluster', 'freq_class_encoded',\n",
      "       'time_frame_char_encoded', 'purchase_trend_encoded',\n",
      "       'day_of_week_encoded', 'most_common_day_encoded', 'segment_id'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list = X_scaled.columns\n",
    "print(feature_list)\n",
    "len(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 30px; font-weight: bold;\">building the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"XGBoostmodel.pkl\", \"rb\") as file:\n",
    "    xgb_clf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.65      0.71     97756\n",
      "           1       0.64      0.77      0.70     79634\n",
      "\n",
      "    accuracy                           0.70    177390\n",
      "   macro avg       0.71      0.71      0.70    177390\n",
      "weighted avg       0.72      0.70      0.70    177390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = xgb_clf.predict(X_scaled)\n",
    "\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 30px; font-weight: bold;\">custom treshhold</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.95      0.79     97756\n",
      "           1       0.88      0.44      0.59     79634\n",
      "\n",
      "    accuracy                           0.72    177390\n",
      "   macro avg       0.78      0.70      0.69    177390\n",
      "weighted avg       0.77      0.72      0.70    177390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_val is your validation dataset\n",
    "y_pred_prob = xgb_clf.predict_proba(X_scaled)[:, 1]  # Probabilities for class 1\n",
    "\n",
    "# Define your custom threshold\n",
    "custom_threshold = 0.8  \n",
    "\n",
    "# Apply the custom threshold to the predicted probabilities\n",
    "y_pred_custom = (y_pred_prob >= custom_threshold).astype(int)\n",
    "\n",
    "print(classification_report(y, y_pred_custom))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
